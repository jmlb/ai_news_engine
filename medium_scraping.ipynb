{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL\n",
    "url_topic = \"https://medium.com/tag/llm/archive\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "response = requests.get(url_topic, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all 'a' tags and extract the 'href' attribute\n",
    "    hrefs = [a.get('href') for a in soup.find_all('a') if a.get('href')]\n",
    "    hrefs = list(set([h for h in hrefs if \"@\" in h]))\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the web page. Status code: {response.status_code}\")\n",
    "    \n",
    "\n",
    "\n",
    "for link in hrefs:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing Mathematical Reasoning in AI: Integrating LLMs with Monte Carlo Tree Search 11h ago\n",
      "Refining the Role of GPT-4 LLM in Virtual Internships 11h ago\n",
      "Literature Review Generation using Llama and Arxiv 12h ago\n",
      "Simplify LLM Quantization Process for Success 12h ago\n",
      "RAG : Q/A from video content using Lang-chain and Mistral-7B 13h ago\n",
      "【学习LangChain】02. 记忆（Memory） 13h ago\n",
      "Master LLM Sentiment Analysis: A Simple Guide 13h ago\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "def extract_post_timing(article):\n",
    "    divs = article.find_all(\"div\")\n",
    "    div_texts = [x.get_text(strip=True) for x in divs]\n",
    "    when = [x for x in div_texts if isinstance(x, str)]\n",
    "    when = [replace_just_now(x) for x in when]\n",
    "\n",
    "    when_ = []\n",
    "    for x in when:\n",
    "        m = extract_time_ago(x)\n",
    "        if len(m) > 0:\n",
    "            when_ += m\n",
    "\n",
    "    when = list(set(when_))\n",
    "    if len(when) > 0:\n",
    "        when = when[0]\n",
    "    else:\n",
    "        print(f\"Failed when ....{div_texts}\\n\")\n",
    "        when = None\n",
    "        \n",
    "    return when\n",
    "\n",
    "def extract_time_ago(input_string):\n",
    "    # Define the regex pattern to match the time format\n",
    "    pattern = r'\\b\\d{1,2}[hd] ago'\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    return matches\n",
    "\n",
    "\n",
    "def replace_just_now(input_str):\n",
    "    if input_str.lower() == \"just now\":\n",
    "        return \"0h ago\"\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def extract_links(this_article):\n",
    "    # extract link\n",
    "    links = [a.get('data-href') for a in this_article.find_all('div') if a.get('data-href')]\n",
    "    if len(links) > 0:\n",
    "        return links[0]\n",
    "    else:\n",
    "        print(f\"Failed links ....{links}\\n\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_snippet(this_article):\n",
    "    # extract short text\n",
    "    h3s = this_article.find_all('h3')\n",
    "    snippet = [h3.get_text(strip=True) for h3 in h3s]\n",
    "    if len(snippet) > 0:\n",
    "        return snippet[0]\n",
    "    else:\n",
    "        print(f\"Failed snippet ....{links}\\n\")\n",
    "        None\n",
    "\n",
    "\n",
    "def extract_title(this_article):\n",
    "        # extract title\n",
    "    h2s = this_article.find_all('h2')\n",
    "    titles = [h2.get_text(strip=True) for h2 in h2s]\n",
    "    \n",
    "    titles_ = [x.get(\"aria-label\") for x in this_article.find_all('div')]\n",
    "    titles_ = [x for x in titles_ if isinstance(x, str)]\n",
    "    if len(titles_) > 0:\n",
    "        title = titles_[0]\n",
    "    elif len(titles) > 0:\n",
    "        title = titles[0]\n",
    "    else:\n",
    "        print(f\"Failed links ....{titles_}\\n\")\n",
    "        title = None\n",
    "\n",
    "    return title\n",
    "\n",
    "def extract_image(this_article):\n",
    "    srcs = list(set([x.get(\"src\") for x in this_article.find_all('img')]))\n",
    "    imgs = [img for img in srcs if \".jpg\" in img.lower() or \".png\" in img.lower() or \".jpeg\" in img.lower()]\n",
    "    if len(imgs) > 0:\n",
    "        return imgs[0]\n",
    "    else:\n",
    "        print(f\"Failed links ....{srcs}\\n\")\n",
    "        return None\n",
    "    return \n",
    "\n",
    "def parsing_article(this_article):\n",
    "\n",
    "    link = extract_links(this_article)\n",
    "    title = extract_title(this_article)\n",
    "    when = extract_post_timing(this_article)\n",
    "    img = extract_image(this_article)\n",
    "    snippet = extract_snippet(this_article)\n",
    "    post = {\"title\": title, \"link\": link, \"img\": img, \"snippet\": snippet, \"when\": when}\n",
    "\n",
    "    return post\n",
    "\n",
    "\n",
    "# Helper function to check if the timestamp is longer than 1 day ago\n",
    "def is_longer_than_1_day(timestamp):\n",
    "    if 'd ago' in timestamp:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Initialize the WebDriver with headless mode\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the Medium tag page\n",
    "url = \"https://medium.com/tag/llm/archive\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait until the page is fully loaded\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "# # Define a function to extract articles\n",
    "def extract_articles(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = soup.find_all('article')\n",
    "    article_data = []\n",
    "    for this_article in articles:\n",
    "        post = parsing_article(this_article)\n",
    "        if detect(post[\"title\"]) != \"en\":\n",
    "            continue\n",
    "        article_data.append(post)\n",
    "    return article_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scroll the page and collect articles\n",
    "collected_articles = []\n",
    "run = True\n",
    "cnt = 1\n",
    "while run:  # Condition to stop scrolling (e.g., 50 articles)\n",
    "    articles = extract_articles(driver)\n",
    "    for article in articles:\n",
    "        ts = article[\"when\"]\n",
    "        print(article['title'], ts)\n",
    "        if is_longer_than_1_day(ts):\n",
    "           run = False\n",
    "        else:\n",
    "            collected_articles.append(article)\n",
    "\n",
    "    # Scroll to the bottom of the page\n",
    "    if len(collected_articles) == 0 and cnt == 1:\n",
    "        run = False\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    cnt += 1\n",
    "    time.sleep(2)  # Wait for new content to load\n",
    "    run = False\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Enhancing Mathematical Reasoning in AI: Integrating LLMs with Monte Carlo Tree Search',\n",
       "  'link': 'https://medium.com/@chaudharysahil379/enhancing-mathematical-reasoning-in-ai-integrating-llms-with-monte-carlo-tree-search-b3ef188cba9a',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:160:107/1*rXEX6IifVU0bhhKBQPBWpQ.png',\n",
       "  'snippet': 'Introduction',\n",
       "  'when': '11h ago'},\n",
       " {'title': 'Refining the Role of GPT-4 LLM in Virtual Internships',\n",
       "  'link': 'https://medium.com/@rishitmayank/refining-the-role-of-gpt-4-llm-in-virtual-internships-f4bdb5798ea3',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:80:53/1*7xaWTdSG9NEJisj2TVdNzg.jpeg',\n",
       "  'snippet': 'The deployment of GPT-4 LLM as a virtual intern not only streamlines mundane tasks but also enriches the learning experience for human…',\n",
       "  'when': '11h ago'},\n",
       " {'title': 'Literature Review Generation using Llama and Arxiv',\n",
       "  'link': 'https://medium.com/@khaoujai/literature-review-generation-using-llama-and-arxiv-8a25aecb3f04',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:80:53/1*RnUuSEEZ85v12_EZi6ajpg.png',\n",
       "  'snippet': 'In the rapidly evolving landscape of scientific research, staying up-to-date with the latest literature can be a daunting task. Researchers…',\n",
       "  'when': '12h ago'},\n",
       " {'title': 'Simplify LLM Quantization Process for Success',\n",
       "  'link': 'https://medium.com/@marketing_novita.ai/simplify-llm-quantization-process-for-success-7126c26633df',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:160:107/0*8xOi1YNI2ICjqzeW.png',\n",
       "  'snippet': 'Simplify the LLM quantization process for success with our expert tips and guidance. Explore our blog for more insights.',\n",
       "  'when': '12h ago'},\n",
       " {'title': 'RAG\\xa0: Q/A from video content using Lang-chain and Mistral-7B',\n",
       "  'link': 'https://medium.com/@nayanika.sarkar98/rag-q-a-from-video-content-using-lang-chain-and-mistral-7b-3714e063010b',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:20:20/1*GyrB9AHZ29zVzcDmtqetBA.png',\n",
       "  'snippet': 'Ever since the popularity of ChatGPT, the Large Language Models (LLMs) are creating a buzz worldwide, and why won’t it? Imagine having a…',\n",
       "  'when': '13h ago'},\n",
       " {'title': '【学习LangChain】02. 记忆（Memory）',\n",
       "  'link': 'https://medium.com/@ai-data-drive/学习langchain-02-记忆-memory-831c43a95149',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:80:53/1*PCDbLHjjEaFFdcIgsKHo0A.png',\n",
       "  'snippet': 'Messages：LangChain 101: 记忆',\n",
       "  'when': '13h ago'},\n",
       " {'title': 'Master LLM Sentiment Analysis: A Simple Guide',\n",
       "  'link': 'https://medium.com/@marketing_novita.ai/master-llm-sentiment-analysis-a-simple-guide-a9692001e780',\n",
       "  'img': 'https://miro.medium.com/v2/resize:fill:160:107/0*blSKDaopPjMEL7Zo.png',\n",
       "  'snippet': 'Master LLM sentiment analysis effortlessly. Explore our simple guide to understanding and implementing LLM sentiment analysis.',\n",
       "  'when': '13h ago'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def valide_post(article, subtopics=[]):\n",
    "    link = article[\"link\"]\n",
    "    if instance(link, type(None)):\n",
    "        return False\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    #options.add_argument('--headless=new')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    #options.add_argument('--no-sandbox')\n",
    "    #options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(link)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tags = soup.find_all('href')\n",
    "    tags = [tag.get('href') for tag in tags]\n",
    "    tags = soup.find_all('a', href=True)\n",
    "    tags = [tag.get('href') for tag in tags]\n",
    "    tags = [tag for tag in tags if \"/tag/\" in tag]\n",
    "    tags = [tag.split(\"?source=post_page\")[0].replace(\"/tag/\", \"\") for tag in tags]\n",
    "    match_tags = [tag for tag in tags if tag in subtopics]\n",
    "\n",
    "    return len(match_tags) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llm', 'mathematical-reasoning']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = soup.find_all('a', href=True)\n",
    "tags = [tag.get('href') for tag in tags]\n",
    "tags = [tag for tag in tags if \"/tag/\" in tag]\n",
    "tags = [tag.split(\"?source=post_page\")[0].replace(\"/tag/\", \"\") for tag in tags]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
